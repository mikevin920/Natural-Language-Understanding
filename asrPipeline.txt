I collected a diverse set of transcripts that include various speech patterns such as sentences spoken at different speeds, 
with stuttering, code-switching, and more. These transcripts were carefully chosen to test the robustness and adaptability of 
our chosen Automatic Speech Recognition (ASR) system.

Below are 10 transcripts for different speaking scenarios:

Word example:
Transcript: "Hippopotamus"
Transcribed text: HIPPOPOTAMUS

Sentence example:
Transcript: "The cat jumped over the fence."
Transcribed text: THE CAT JUMPED OVER THE FENCE

Sentence read very slowly:
Transcript: "The... weather... today... is... exceptionally... sunny."
Transcribed text: THE WEATHER TO DAY IS EXCEPTIONALLY SUNNY

Sentence read at normal speed, except speeding up at some words:
Transcript: "The weather today is excep-tion-ally sunny and warm."
Transcribed text: THE WEATHER TO DAY IS EXCEPTIONALLY SUNNY AND WARM

Sentence spoken very fast:
Transcript: "Canyoubelievethisweatherwe'rehavingtoday?"
Transcribed text: HE BELIEVES WHERE WE'RE HAVING TO DAY

Sentence containing slang or words from a second language (code-switching):
Transcript: Hey bro, the 烤鸭 (kao ya) the other day at the restaurant tasted amazing, no cap.
Transcribed text: ABROAD TO KARA YADA ADAH AT THE RESTAURANT TASTED AMAZING NO CAT

Sentence with one word spelled out by alphabet:
Transcript: "My favorite color is B-L-U-E."
Transcribed text: HIS FAVORITE COLOR IS B L U E

Sentence where two neighboring words have blurred boundaries:
Transcript: "I'm goingto the store to buy some groceries for tonight's dinner.
Transcribed text: THE WEATHER IS PERFECT FOR A DAY AT THE BEACH DON'T YOU THINK

Sentence spoken with some stuttering, filler words, and repetitiveness:
Transcript: "I, um, think that, you know, we should, like, definitely go to, to the park today."
Transcribed text: I DON'T THINK THAT YOU KNOW DON'T WE SHOULD LIKE DON'T YOU GO TO TO DEPART TO DAY

Procedures to collect the audio files:

To collect sounds for the 10 transcripts above, I used my iPhone 11’s app voice memos and recorded myself reading the 
transcripts. Then, I uploaded the m4a file to my colab notebook to transcribe the audio recorded.

ASR system:

I used SpeechBrain’s ASR model trained on the LibriSpeech dataset. The model architecture consists of an encoder-decoder 
structure with a CRDNN encoder and an RNNLM decoder. LibriSpeech is a large-scale dataset containing 1,000 hours of English 
speech derived from audiobooks. 

Sample code used in colab notebook:

 !pip install speechbrain

from speechbrain.pretrained import EncoderDecoderASR

asr_model = EncoderDecoderASR.from_hparams(source="speechbrain/asr-crdnn-rnnlm-librispeech", 
savedir="pretrained_models/asr-crdnn-rnnlm-librispeech")

fast_sentence = asr_model.transcribe_file(/5. Sentence spoken very fast.m4a')
print(fast_sentence)

ASR System Performance:

Upon evaluating the performance of the ASR system on the transcripts prepared, I observed the following:

The system performed well in accurately transcribing speech spoken at a normal pace and with clear enunciation. Speech with words
spelled out were also transcribed perfectly. Sentences spoken at a slow pace and faster pace were transcribed reasonably well, 
transcribing certain words to similar sounding words.

However, It faced challenges when dealing with sentences with code-switching and slang. Since the dataset was only trained on 
English audio books, it is the expected behavior because slangs and other languages were not part of the training dataset.
It also has difficulty dealing with speech that includes stuttering, filler words, or repetitive phrases. It occasionally 
misinterpreted or omitted these elements in the transcriptions. In cases where neighboring words had blurred boundaries or when sentences were spoken very fast, the system's performance was 
less accurate, it merges the words together.

In conclusion, the SpeechBrain system performed well in general, but faced difficulties in specific scenarios where the dataset
wasn't trained on. Further improvements could be made by fine tuning a pre-trained model using datasets specifically prepared 
for certain tasks.



