{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 4: Large Language Model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, random, numpy as np, argparse, sys, re, os\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import classification_report, f1_score, recall_score, accuracy_score\n",
    "\n",
    "# change it with respect to the original model\n",
    "from tokenizer import BertTokenizer\n",
    "from bert import BertModel\n",
    "from tqdm import tqdm\n",
    "from classifier import BertSentClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***4.1 The fine-tuned minBERT classifier***\n",
    "\n",
    "\n",
    "Instantiate a BertSentClassifier model. Load the state dict from the checkpoint saved from the\n",
    "previous question."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the best model checkpoint from the previous question. and instantiate a BertSentClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\n",
    "    'cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "filepath = 'flexible-50-5e-06.pt'\n",
    "\n",
    "saved = torch.load(filepath)\n",
    "config = saved['model_config']\n",
    "model = BertSentClassifier(config)\n",
    "model.load_state_dict(saved['model'])\n",
    "model = model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifiy the batch of sentences\n",
    "\n",
    "- Very positive: The movie was a masterpiece with a brilliant storyline and exceptional performances by the entire cast.\n",
    "\n",
    "- Less positive: The film was enjoyable overall, but some of the dialogues felt a bit clichéd.\n",
    "\n",
    "- Slightly negative: The plot was somewhat predictable, and it failed to capture my interest throughout the movie.\n",
    "\n",
    "- Very negative: It was a terrible movie with poor acting, a weak script, and subpar visual effects that made it unbearable to watch.\n",
    "\n",
    "- Off-the-topic: I had a delicious sandwich for lunch from the new café around the corner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"The movie was a masterpiece with a brilliant storyline and exceptional performances by the entire cast.\",\n",
    "    \"The film was enjoyable overall, but some of the dialogues felt a bit clichéd.\",\n",
    "    \"The plot was somewhat predictable, and it failed to capture my interest throughout the movie.\",\n",
    "    \"It was a terrible movie with poor acting, a weak script, and subpar visual effects that made it unbearable to watch.\",\n",
    "    \"I had a delicious sandwich for lunch from the new café around the corner.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_data(data):\n",
    "    sents = [x[0] for x in data]\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    encoding = tokenizer(sents, return_tensors='pt', padding=True, truncation=True)\n",
    "    token_ids = torch.LongTensor(encoding['input_ids'])\n",
    "    attention_mask = torch.LongTensor(encoding['attention_mask'])\n",
    "    token_type_ids = torch.LongTensor(encoding['token_type_ids'])\n",
    "\n",
    "    return token_ids, token_type_ids, attention_mask, sents\n",
    "\n",
    "\n",
    "# create the data which is a list of (sentence, label, token for the labels)\n",
    "def create_data(sentences):\n",
    "    # specify the tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    data = []\n",
    "\n",
    "    for line in sentences:\n",
    "        sent = line.lower().strip()\n",
    "        tokens = tokenizer.tokenize(\"[CLS] \" + sent + \" [SEP]\")\n",
    "        data.append((sent,tokens))\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = create_data(sentences=sentences)\n",
    "\n",
    "token_ids, type_ids, mask, sents = pad_data(data)\n",
    "\n",
    "logits = model(token_ids, mask)\n",
    "logits = logits.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in enumerate(logits):\n",
    "    row = np.exp(row)  # convert to probability from logsofrmax\n",
    "    positive = row[0] <= row[1]\n",
    "    print(sentences[i] + '|||' + ('Positive' if positive else 'Negative'))\n",
    "    print('Positive Probability: ' + str(row[1]))\n",
    "    print(' Negative Probability: ' + str(row[0]))\n",
    "    print('------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
