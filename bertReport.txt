Runtime: Below is runtime for per epoch and the breakdown of each individual step

Frozen model:
    - Train: 28 seconds
    - Eval: 27 seconds
    - Eval-2: 3 seconds

Flexible model:
    - Train: 79 seconds
    - Eval: 27 seconds
    - Eval-2: 3 seconds

Computation Hardware:
    - CPU : Intel(R) Core(TM) i7-10700F CPU @ 2.90GHz
    - GPU: NVIDIA GeForce RTX 3060, 12GB VRAM
    - RAM : 16GB

Hyperparameters of the best frozen model:
option: frozen
batch size: 8
learning rate: 1e-3
hidden_dropout_prob: 0.2
epochs: 50

Results of the frozen model training shown below:
Best Training accuracy: 83.7%
Best Dev accuracy: 84.5%
Test accuracy: N/A, since test labels are not provided
Best model saved at epoch 35.

Command used to run the code for the frozen models:
  	python classifier.py --use_gpu --lr 1e-3 --hidden_dropout_prob 0.2 --epochs 100

Hyperparameters Tuning Procedure for Frozen model:

The initial model with default hyperparameters was trained for 10 epochs using a dropout rate of 0.3, 
a learning rate of 1e-5, and a batch size of 8. However, it did not fully converge, as evidenced by the steadily 
increasing dev set accuracy for 10 epochs. The model achieved a dev set accuracy of 60.0%.

Tuning Strategy:

I used the incremental tuning strategy to incrementally make changes to the hyperparameters. The initial dropout rate 
of 0.3 might be too high, causing the model to underfit the data, so I decreased the dropout rate to 0.2 to 
allow the model to retain more information during training. I also increased the number of epochs to 50 to 
allow the model more opportunities to learn from the data since it didn't fully converge with 10 epochs. 
The initial learning rate of 1e-5 may be too low, resulting in slow convergence. I increased the rate to 1e-3 so the 
model can take larger steps, which speeds up the convergence process and potentially allows the model to reach a better
optimum. The rest of the hyperparameters were kept at default. The best model increased the dev set accuracy to 84.5% 
at epoch 35.

Hyperparameters of the best flexible model:
    - option: flexible
    - batch size: 8
    - learning rate: 1e-5
    - hidden_dropout_prob: 0.3
    - epochs: 10

Results of the flexible model training shown below:
Best Training accuracy: 99.9%
Best Dev accuracy: 96.7%
Test accuracy: N/A, since test labels are not provided
Best result saved at epoch 5.

Command used to run the code for the flexible models:
python classifier.py --use_gpu --lr 5e-6 --epochs 10 --option flexible
python classifier.py --use_gpu --option flexible --lr 1e-5 --hidden_dropout_prob 0.3 --epochs 10 --option flexible

Hyperparameters Tuning Procedure for Flexible model:

The initial model with default hyperparameters was trained for 10 epochs using a dropout rate of 0.3, a 
learning rate of 1e-5, and a batch size of 8. The initial model achieved dev set accuracy of 96.7% at epoch 5, 
which means the default hyperparameters are sufficient baseline.

Tuning Strategy:

To further increase the dev accuracy, I tried conducting a grid search using the following parameters
 param_grid = {
    "learning_rate": [1e-5, 5e-6],
    "hidden_dropout_prob": [0.15, 0.2, 0.3],
    "epochs": [10, 50],
}
I tried decreased the learning rate to 5e-6 to improve generalization and help the model make further progress 
in optimizing its parameter since the train loss plateaued at around epoch 4. 
I also increased the number of epochs to 50 to allow further training and fine-tuning of the models.
I experimented with three different dropout probabilities [0.15, 0.2 0.3]. However, the best model found experimentally
was still the initial model with learning rate 1e-05, hidden_dropout_probability 0.3, batch size: 8 and number of epoch 10. 
The best dev set accuracy was 96.7%.




