Runtime: Below is runtime for per epoch and the breakdown of each individual step

Frozen model:
    - Train: 28 seconds
    - Eval: 27 seconds
    - Eval-2: 3 seconds

Flexible model:
    - Train: 79 seconds
    - Eval: 27 seconds
    - Eval-2: 3 seconds

Computation Hardware:
    - CPU : Intel(R) Core(TM) i7-10700F CPU @ 2.90GHz
    - GPU: NVIDIA GeForce RTX 3060, 12GB VRAM
    - RAM : 16GB

Hyperparameters of the best frozen model:
option: frozen
batch size: 8
learning rate: 1e-3
hidden_dropout_prob: 0.2
epochs: 50

Results of the frozen model training shown below:
Best Training accuracy: 83.7%
Best Dev accuracy: 84.5%
Test accuracy: N/A, since test labels are not provided
Best model saved at epoch 35.

Command used to run the code for the frozen models:
  	python classifier.py --use_gpu --lr 1e-3 --hidden_dropout_prob 0.2 --epochs 100

Hyperparameters Tuning Procedure for Frozen model:

The initial model with default hyperparameters was trained for 10 epochs using a dropout rate of 0.3, 
a learning rate of 1e-5, and a batch size of 8. However, it did not fully converge, as evidenced by the steadily 
increasing dev set accuracy for 10 epochs. The model achieved a dev set accuracy of 60.0% and a training set accuracy 
of 71.3%.

Tuning Strategy:

I used the incremental tuning strategy to incrementally make changes to the hyperparameters. The initial dropout rate 
of 0.3 might be too high, causing the model to underfit the data, so I decreased the dropout rate to 0.2 to 
allow the model to retain more information during training. I also increased the number of epochs to 50 to 
allow the model more opportunities to learn from the data since it didn't fully converge with 10 epochs. 
The initial learning rate of 1e-5 may be too low, resulting in slow convergence. I increased the rate to 1e-3 so the 
model can take larger steps, which speeds up the convergence process and potentially allows the model to reach a better
optimum. The rest of the hyperparameters were kept at default. The best model increased the train accuracy to 83.7% and 
dev set accuracy to 84.5% at epoch 35.

Hyperparameters of the best flexible model:
    - option: flexible
    - batch size: 8
    - learning rate: 5e-6
    - hidden_dropout_prob: 0.3
    - epochs: 50

Results of the flexible model training shown below:
Best Training accuracy: 99.9%
Best Dev accuracy: 0.963
Test accuracy: N/A, since test labels are not provided
Best result saved at epoch 14.

Command used to run the code for the flexible models:
python classifier.py --use_gpu --lr 1e-6 --hidden_dropout_prob 0.1 --epochs 50 --option flexible

Hyperparameters Tuning Procedure for Flexible model:

The initial model with default hyperparameters was trained for 10 epochs using a dropout rate of 0.3, a 
learning rate of 1e-5, and a batch size of 8.  The initial model achieved dev set accuracy of 96.7% at epoch 5, 
which means the default hyperparameters are sufficient baseline.

Tuning Strategy:

To further increase the dev accuracy, I decreased the learning rate to 5e-6 to improve generalization and help 
the model make further progress in optimizing its parameter since the train loss plateaued at around epoch 4. 
I also increased the number of epochs to 50 to allow further training and fine-tuning of the models.
I experimented with three different dropout probabilities [0.15, 0.2 0.3] and found experimentally that the 
0.15 dropout probability gives the best model performance. The best model increased the
dev set accuracy to 84.5% at epoch 35.




